{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT9fqQhd8yiu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[classic_control]\n",
        "!pip install gymnasium[mujoco]"
      ],
      "metadata": {
        "id": "ec8Nxlj0YKIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Exe:\", sys.executable)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torch CUDA runtime:\", torch.version.cuda)\n",
        "print(\"cuDNN available:\", torch.backends.cudnn.is_available())\n",
        "print(\"CUDA visible devices:\", torch.cuda.device_count(), torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "qCaKlWycpG8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "1Wg5eA6pYvFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AI assisted, code taken from source: https://www.geeksforgeeks.org/deep-learning/reinforcement-learning-using-pytorch/\n",
        "#How do I get a default NN model from pytorch that I can use as a value function estimator for my RL algorithm code\n",
        "#extension to Continuous action space done with AI assistance\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical, Independent, Normal, TransformedDistribution\n",
        "from torch.distributions.transforms import TanhTransform, AffineTransform\n",
        "class actor_critic_cts(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, hidden = 128, continuous = False, action_low = None, action_high = None, init_logstd = 0.0):\n",
        "    super().__init__()\n",
        "    self.continuous = continuous\n",
        "    self.pi_net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, hidden),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "    if not self.continuous:\n",
        "      self.pi_head = nn.Linear(hidden, action_dim)\n",
        "    else:\n",
        "      self.mu_head = nn.Linear(hidden, action_dim)\n",
        "      self.log_std = nn.Parameter(torch.full((action_dim,), init_logstd))\n",
        "      action_low  = torch.as_tensor(action_low,  dtype=torch.float32)\n",
        "      action_high = torch.as_tensor(action_high, dtype=torch.float32)\n",
        "      self.register_buffer(\"action_loc\",   (action_high + action_low) / 2.0) #AI\n",
        "      self.register_buffer(\"action_scale\", (action_high - action_low) / 2.0) #AI\n",
        "\n",
        "    self.v_net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, 1)\n",
        "    )\n",
        "\n",
        "  def policy(self, state):\n",
        "    h = self.pi_net(state)\n",
        "    if not self.continuous:\n",
        "      logits = self.pi_head(h)\n",
        "      return Categorical(logits=logits)\n",
        "    else:\n",
        "      mu = self.mu_head(h)\n",
        "      std = self.log_std.exp().expand_as(mu)\n",
        "      base = Independent(Normal(mu, std), 1)\n",
        "      return TransformedDistribution(  #AI\n",
        "        base,\n",
        "        [TanhTransform(cache_size=1),\n",
        "         AffineTransform(loc=self.action_loc, scale=self.action_scale)]\n",
        "      )\n",
        "\n",
        "  def value(self, state):\n",
        "    return self.v_net(state).squeeze(-1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    dist = self.policy(state)\n",
        "    V = self.value(state)\n",
        "    return dist, V"
      ],
      "metadata": {
        "id": "M_MKPUhqmTSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AI assisted, code taken from source: https://www.geeksforgeeks.org/deep-learning/reinforcement-learning-using-pytorch/\n",
        "#How do I get a default NN model from pytorch that I can use as a value function estimator for my RL algorithm code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "class actor_critic_dsc(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, hidden = 128):\n",
        "    super().__init__()\n",
        "    self.pi_net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, action_dim)\n",
        "    )\n",
        "    self.v_net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, hidden),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden, 1)\n",
        "    )\n",
        "\n",
        "  def policy(self, state):\n",
        "    logits = self.pi_net(state)\n",
        "    return Categorical(logits=logits)\n",
        "\n",
        "  def value(self, state):\n",
        "    return self.v_net(state).squeeze(-1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    dist = self.policy(state)\n",
        "    V = self.value(state)\n",
        "    return dist, V"
      ],
      "metadata": {
        "id": "DMr1vuIeTTgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_state(env):\n",
        "  s, _  = env.reset()\n",
        "  return s"
      ],
      "metadata": {
        "id": "T7xdlM1Up_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GAE(r_hist, v_hist, next_val, traj_ends, y = 0.99, l = 0.95, terms = 150):\n",
        "  r_hist = torch.tensor(r_hist, dtype=torch.float32)\n",
        "  v_hist = torch.tensor(v_hist, dtype=torch.float32)\n",
        "  traj_ends = torch.tensor(traj_ends, dtype=torch.float32)\n",
        "  T = len(r_hist)\n",
        "  v_tplus1 = torch.empty(T, dtype=torch.float32)\n",
        "  if T > 1:\n",
        "    v_tplus1[:-1] = v_hist[1:]\n",
        "  if traj_ends[-1] == 1.0:\n",
        "    v_tplus1[-1] = 0.0\n",
        "  else:\n",
        "    v_tplus1[-1] = next_val\n",
        "  delta_in_t = r_hist + y*(1-traj_ends)*v_tplus1 - v_hist\n",
        "  GAE_tensor = torch.zeros(T, dtype=torch.float32)\n",
        "  for t in range(T):\n",
        "    GAE_t = 0.0\n",
        "    A_t_i = 0.0\n",
        "    stop = 1.0\n",
        "    y_factor = 1.0\n",
        "    for i in range(1, 1 + min(T-t, terms)):\n",
        "      A_t_i += stop*y_factor*delta_in_t[t + i-1]\n",
        "      GAE_t += (l**(i-1))*A_t_i\n",
        "      stop = stop * (1.0 - traj_ends[t + i-1])\n",
        "      y_factor *= y\n",
        "      if stop == 0.0:\n",
        "          break\n",
        "    GAE_tensor[t] = (1-l)*GAE_t\n",
        "  return GAE_tensor\n"
      ],
      "metadata": {
        "id": "F7X9QnZdJcRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C A R T P O L E\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "total_timesteps = 4000 #timesetps taken in each epoch\n",
        "traj_max_timesteps = 1000\n",
        "num_epochs = 40\n",
        "num_GAE_terms = 150\n",
        "stepsize_w = 1e-3\n",
        "stepsize_theta = 3e-3\n",
        "run_start_time = time.time()\n",
        "best_mean_return = -float(\"inf\")\n",
        "net = actor_critic_dsc(state_dim, action_dim, hidden=128).to(\"cpu\")\n",
        "\n",
        "w = list(net.v_net.parameters())\n",
        "theta = list(net.pi_net.parameters())\n",
        "e_clip = 0.2\n",
        "\n",
        "\"\"\"\n",
        "SGD(lr=0.5) on both actor and critic is extremely high for VPG with neural nets\n",
        "and categorical policies, and plain SGD here is brittle.\n",
        "Switching to Adam with small lrs is a vanilla change (not clipping/normalization)\n",
        "and typically the difference between crawling and clean convergence.\n",
        "\"\"\"\n",
        "actor_update = torch.optim.Adam(theta, lr=stepsize_theta)\n",
        "critic_update = torch.optim.Adam(w, lr=stepsize_w)\n",
        "\n",
        "all_traj_returns = []          # mean reward per trajectory (sum/length)\n",
        "all_traj_epochs = []         # epoch index for each trajectory\n",
        "epoch_spans = []\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    epoch_states, epoch_actions, epoch_vals, epoch_adv, epoch_returns = [], [], [], [], [] # data for each trajectory roll-out\n",
        "    epoch_t = 0;\n",
        "    epoch_logp = []\n",
        "    traj_returns = []\n",
        "    epoch_start_idx = len(all_traj_returns)\n",
        "    while epoch_t < total_timesteps:\n",
        "      s = reset_state(env)\n",
        "      traj_states, traj_actions, traj_vals, traj_rews, traj_ends = [], [], [], [], [] # data for each trajectory roll-out\n",
        "      done = False\n",
        "      traj_t = 0\n",
        "      while (not done) and (traj_t<traj_max_timesteps):\n",
        "        s = torch.as_tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          dist, V = net(s)\n",
        "          a = dist.sample()\n",
        "          logp_a = dist.log_prob(a)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a.item())\n",
        "        done = terminated or truncated #use this instead of a defined for loop, because in gymnasium the trajectory might terminate in the environment when some success/failure conditions are met\n",
        "        traj_rews.append(r)\n",
        "        traj_states.append(s.squeeze(0))\n",
        "        traj_vals.append(V.item())\n",
        "        traj_actions.append(a.item())\n",
        "        epoch_logp.append(logp_a.item())\n",
        "        \"\"\"terminated = true environment terminal (pole fell) → mask it.\n",
        "            truncated = time limit → do not mask; bootstrap with V_sT\n",
        "        \"\"\"\n",
        "        traj_ends.append(1.0 if terminated else 0.0)\n",
        "        s = s_next\n",
        "        traj_t += 1\n",
        "        epoch_t += 1\n",
        "      # AI IDENTIFIED FIXES\n",
        "        if epoch_t >= total_timesteps:      # FIX: stop collecting if epoch budget reached\n",
        "            break\n",
        "      # ---- prevent ends[-1] crash if trajectory collected 0 steps ----\n",
        "      if len(traj_rews) == 0:               # FIX: skip empty trajectories\n",
        "        continue\n",
        "      rews  = torch.tensor(traj_rews,  dtype=torch.float32, device=device)\n",
        "      vals  = torch.tensor(traj_vals,  dtype=torch.float32, device=device)\n",
        "      ends = torch.tensor(traj_ends, dtype=torch.float32, device=device)\n",
        "\n",
        "      if ends[-1] == 1.0:  #implementation and use of dones to handle the ends of trajectory by truncation or termination was implemented with ChatGPT 5 assistance\n",
        "        next_v = 0.0\n",
        "      else:\n",
        "        s_tplus1 = torch.as_tensor(s, dtype=torch.float32, device=\"cpu\").unsqueeze(0) #obtain the state after the last action taken before trajectory ended\n",
        "        with torch.no_grad(): #AI provided. Used so that our value function is a constant, and estimated strictly through NN output, and does not give along the graph\n",
        "                              #that generated the next_v\n",
        "                              #using .item() does ensure in part that the graph is not given along,\n",
        "                              #but no_grad() allows us to have entirely skip the computation of the generation graph\n",
        "            next_v = net.value(s_tplus1).item()\n",
        "\n",
        "      A = GAE(traj_rews, traj_vals, next_v, traj_ends)\n",
        "      ret = A + torch.tensor(traj_vals, dtype=torch.float32)\n",
        "\n",
        "      epoch_states.extend(traj_states)\n",
        "      epoch_actions.extend(traj_actions)\n",
        "      epoch_vals.extend(traj_vals)\n",
        "      epoch_adv.extend(A.tolist())\n",
        "      epoch_returns.extend(ret.tolist())\n",
        "\n",
        "      traj_len = len(traj_rews)\n",
        "      # after a trajectory finishes\n",
        "      traj_return = rews.sum().item()             # <-- sum, not mean\n",
        "      traj_returns.append(traj_return)\n",
        "\n",
        "      all_traj_returns.append(traj_return)        # rename your list\n",
        "      all_traj_epochs.append(k)\n",
        "\n",
        "    if len(epoch_states) == 0:\n",
        "      print(\"empty epoch; continuing\")\n",
        "      continue\n",
        "\n",
        "    mean_trajreward = float(np.mean(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    std_trajreward  = float(np.std(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    min_trajreward  = float(np.min(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    max_trajreward  = float(np.max(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "\n",
        "    state_t = torch.stack(epoch_states).to(torch.float32).to(device)\n",
        "    action_t = torch.tensor(epoch_actions, dtype=torch.int64, device=device)\n",
        "    adv_t = torch.tensor(epoch_adv, dtype=torch.float32, device=device )\n",
        "    ret_t = torch.tensor(epoch_returns, dtype=torch.float32, device=device)\n",
        "    logp_old_t = torch.tensor(epoch_logp, dtype=torch.float32, device=device)\n",
        "\n",
        "    v_step = net.value(state_t).squeeze(-1)                                  # V(s_t) (with grad)\n",
        "    value_loss = torch.nn.functional.mse_loss(v_step, ret_t.detach())\n",
        "    critic_update.zero_grad(); value_loss.backward(); critic_update.step()\n",
        "\n",
        "\n",
        "    dist = net.policy(state_t)\n",
        "    logp_t = dist.log_prob(action_t)\n",
        "    r_th = torch.exp(logp_t - logp_old_t)\n",
        "    L_epsilon = -torch.min(r_th*adv_t.detach(),\n",
        "                           torch.clamp(r_th, 1.0 - e_clip, 1.0 + e_clip)*adv_t.detach()).mean()\n",
        "    actor_update.zero_grad(); L_epsilon.backward(); actor_update.step()\n",
        "\n",
        "    pi_loss_val = float(L_epsilon.item())\n",
        "    v_loss_val  = float(value_loss.item())\n",
        "\n",
        "    elapsed = time.time() - run_start_time\n",
        "    if mean_trajreward > best_mean_return:\n",
        "        best_mean_return = mean_trajreward\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": net.state_dict(),\n",
        "                \"actor_opt\": actor_update.state_dict(),\n",
        "                \"critic_opt\": critic_update.state_dict(),\n",
        "                \"epoch\": k,\n",
        "                \"best_mean_return\": best_mean_return,\n",
        "            },\n",
        "            \"actor_critic_best1.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "    f\"epoch {k+1:03d}/{num_epochs} | steps(epoch) ~{len(epoch_returns):5d} \"\n",
        "    f\"| return μ {mean_trajreward:6.2f} ± {std_trajreward:5.2f} (min {min_trajreward:5.1f}, max {max_trajreward:5.1f}) \"\n",
        "    f\"| pi_loss {pi_loss_val:7.4f} | v_loss {v_loss_val:7.4f} \"\n",
        "    f\"| best μ {best_mean_return:6.2f} | {elapsed:6.1f}s\")\n",
        "    epoch_end_idx = len(all_traj_returns)   # one past the last traj index of this epoch\n",
        "    epoch_mean_for_plot = float(np.mean(all_traj_returns[epoch_start_idx:epoch_end_idx])) if epoch_end_idx > epoch_start_idx else np.nan\n",
        "    epoch_spans.append((epoch_start_idx, epoch_end_idx, epoch_mean_for_plot))\n",
        "# x-axis = global trajectory index\n",
        "x = np.arange(len(all_traj_returns))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1) per-trajectory returns (blue)\n",
        "traj_line, = plt.plot(x, all_traj_returns, linewidth=1.0, label=\"trajectory return\")\n",
        "\n",
        "# 2) per-epoch mean as a red step line (built from epoch_spans)\n",
        "xs, ys = [], []\n",
        "for (s, e, m) in epoch_spans:\n",
        "    if e > s and np.isfinite(m):\n",
        "        xs += [s, e]\n",
        "        ys += [m, m]\n",
        "epoch_step = None\n",
        "if xs:\n",
        "    epoch_step = plt.step(xs, ys, where=\"post\", color=\"red\", linewidth=2.5, label=\"epoch mean return\")\n",
        "\n",
        "# (optional) faint epoch boundaries — no legend for these\n",
        "for (s, e, _) in epoch_spans:\n",
        "    plt.axvline(e - 0.5, alpha=0.1, linewidth=1)\n",
        "\n",
        "# 3) mark best epoch mean with a black star\n",
        "best_scatter = None\n",
        "valid = [(i, s, e, m) for i, (s, e, m) in enumerate(epoch_spans) if np.isfinite(m) and e > s]\n",
        "if valid:\n",
        "    i_best, s_best, e_best, m_best = max(valid, key=lambda t: t[3])\n",
        "    x_best = 0.5 * (s_best + e_best - 1)\n",
        "    best_scatter = plt.scatter([x_best], [m_best], marker='*', s=140, color='black', zorder=6, label='best epoch mean')\n",
        "    plt.annotate(f\"{m_best:.1f}\", xy=(x_best, m_best), xytext=(8, 8),\n",
        "                 textcoords=\"offset points\", fontsize=9,\n",
        "                 arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
        "\n",
        "plt.xlabel(\"Trajectory index (across all epochs)\")\n",
        "plt.ylabel(\"Return (sum of rewards)\")\n",
        "plt.title(\"Per-trajectory returns (blue) with per-epoch mean (red step)\")\n",
        "plt.legend()  # uses labels set above\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2_mlEm8L_Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I N V E R T E D  P E N D U L U M\n",
        "\n",
        "env = gym.make(\"InvertedPendulum-v5\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_space = env.action_space\n",
        "total_timesteps = 4000 #timesetps taken in each epoch\n",
        "traj_max_timesteps = 1001\n",
        "num_epochs = 40\n",
        "num_GAE_terms = 150\n",
        "stepsize_w = 1e-3\n",
        "stepsize_theta = 1e-3\n",
        "run_start_time = time.time()\n",
        "best_mean_return = -float(\"inf\")\n",
        "net = actor_critic_cts(state_dim, action_dim = action_space.shape[0], hidden=128, continuous = True, action_low=action_space.low, action_high=action_space.high).to(\"cpu\")\n",
        "\n",
        "w = list(net.v_net.parameters())\n",
        "e_clip = 0.2\n",
        "\n",
        "\"\"\"\n",
        "SGD(lr=0.5) on both actor and critic is extremely high for VPG with neural nets\n",
        "and categorical policies, and plain SGD here is brittle.\n",
        "Switching to Adam with small lrs is a vanilla change (not clipping/normalization)\n",
        "and typically the difference between crawling and clean convergence.\n",
        "\"\"\"\n",
        "actor_params = list(net.pi_net.parameters())\n",
        "if net.continuous:\n",
        "    actor_params += list(net.mu_head.parameters()) + [net.log_std]\n",
        "else:\n",
        "    actor_params += list(net.pi_head.parameters())\n",
        "actor_update = torch.optim.Adam(actor_params, lr=stepsize_theta)\n",
        "\n",
        "critic_update = torch.optim.Adam(w, lr=stepsize_w)\n",
        "\n",
        "all_traj_returns = []          # mean reward per trajectory (sum/length)\n",
        "all_traj_epochs = []         # epoch index for each trajectory\n",
        "epoch_spans = []\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    epoch_states, epoch_actions, epoch_vals, epoch_adv, epoch_returns = [], [], [], [], [] # data for each trajectory roll-out\n",
        "    epoch_t = 0;\n",
        "    epoch_logp = []\n",
        "    traj_returns = []\n",
        "    while epoch_t < total_timesteps:\n",
        "      s = reset_state(env)\n",
        "      traj_states, traj_actions, traj_vals, traj_rews, traj_ends = [], [], [], [], [] # data for each trajectory roll-out\n",
        "      done = False\n",
        "      traj_t = 0\n",
        "      epoch_start_idx = len(all_traj_returns)\n",
        "      while (not done) and (traj_t<traj_max_timesteps):\n",
        "        s = torch.as_tensor(s, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          dist, V = net(s)\n",
        "          a = dist.sample()\n",
        "          logp_a = dist.log_prob(a)\n",
        "        a_env = a.squeeze(0).cpu().numpy().astype(np.float32)  #AI\n",
        "        s_next, r, terminated, truncated, _ = env.step(a_env)\n",
        "        done = terminated or truncated #use this instead of a defined for loop, because in gymnasium the trajectory might terminate in the environment when some success/failure conditions are met\n",
        "        traj_rews.append(r)\n",
        "        traj_states.append(s.squeeze(0))\n",
        "        traj_vals.append(V.item())\n",
        "        traj_actions.append(a.squeeze(0))\n",
        "        epoch_logp.append(logp_a.squeeze(0))\n",
        "        \"\"\"terminated = true environment terminal (pole fell) → mask it.\n",
        "            truncated = time limit → do not mask; bootstrap with V_sT\n",
        "        \"\"\"\n",
        "        traj_ends.append(1.0 if terminated else 0.0)\n",
        "        s = s_next\n",
        "        traj_t += 1\n",
        "        epoch_t += 1\n",
        "      # AI IDENTIFIED FIXES\n",
        "        if epoch_t >= total_timesteps:      # FIX: stop collecting if epoch budget reached\n",
        "            break\n",
        "      # ---- prevent ends[-1] crash if trajectory collected 0 steps ----\n",
        "      if len(traj_rews) == 0:               # FIX: skip empty trajectories\n",
        "        continue\n",
        "      rews  = torch.tensor(traj_rews,  dtype=torch.float32)\n",
        "      vals  = torch.tensor(traj_vals,  dtype=torch.float32)\n",
        "      ends = torch.tensor(traj_ends, dtype=torch.float32)\n",
        "\n",
        "      if ends[-1] == 1.0:  #implementation and use of dones to handle the ends of trajectory by truncation or termination was implemented with ChatGPT 5 assistance\n",
        "        next_v = 0.0\n",
        "      else:\n",
        "        s_tplus1 = torch.as_tensor(s, dtype=torch.float32, device=\"cpu\").unsqueeze(0) #obtain the state after the last action taken before trajectory ended\n",
        "        with torch.no_grad(): #AI provided. Used so that our value function is a constant, and estimated strictly through NN output, and does not give along the graph\n",
        "                              #that generated the next_v\n",
        "                              #using .item() does ensure in part that the graph is not given along,\n",
        "                              #but no_grad() allows us to have entirely skip the computation of the generation graph\n",
        "            next_v = net.value(s_tplus1).item()\n",
        "\n",
        "      A = GAE(traj_rews, traj_vals, next_v, traj_ends)\n",
        "      ret = A + torch.tensor(traj_vals, dtype=torch.float32)\n",
        "\n",
        "      epoch_states.extend(traj_states)\n",
        "      epoch_actions.extend(traj_actions)\n",
        "      epoch_vals.extend(traj_vals)\n",
        "      epoch_adv.extend(A.tolist())\n",
        "      epoch_returns.extend(ret.tolist())\n",
        "\n",
        "      traj_len = len(traj_rews)\n",
        "      # after a trajectory finishes\n",
        "      traj_return = rews.sum().item()             # <-- sum, not mean\n",
        "      traj_returns.append(traj_return)\n",
        "\n",
        "      all_traj_returns.append(traj_return)        # rename your list\n",
        "      all_traj_epochs.append(k)\n",
        "\n",
        "    if len(epoch_states) == 0:\n",
        "      print(\"empty epoch; continuing\")\n",
        "      continue\n",
        "\n",
        "    mean_trajreward = float(np.mean(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    std_trajreward  = float(np.std(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    min_trajreward  = float(np.min(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    max_trajreward  = float(np.max(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "\n",
        "    state_t = torch.stack(epoch_states).to(torch.float32)\n",
        "    action_t = torch.stack(epoch_actions).to(torch.float32)\n",
        "    adv_t = torch.tensor(epoch_adv, dtype=torch.float32)\n",
        "    ret_t = torch.tensor(epoch_returns, dtype=torch.float32)\n",
        "    logp_old_t = torch.tensor(epoch_logp, dtype=torch.float32)\n",
        "\n",
        "    v_step = net.value(state_t).squeeze(-1)                                  # V(s_t) (with grad)\n",
        "    value_loss = torch.nn.functional.mse_loss(v_step, ret_t.detach())\n",
        "    critic_update.zero_grad(); value_loss.backward(); critic_update.step()\n",
        "\n",
        "    # Clamp actions into open interval so atanh is well-defined when re-computing log_prob\n",
        "    ###AIstart\n",
        "    eps = 1e-6\n",
        "    low  = (net.action_loc - net.action_scale + eps).unsqueeze(0)\n",
        "    high = (net.action_loc + net.action_scale - eps).unsqueeze(0)\n",
        "    action_t_clamped = torch.max(torch.min(action_t, high), low)\n",
        "    ###AIend\n",
        "\n",
        "    dist = net.policy(state_t)\n",
        "    logp_t = dist.log_prob(action_t_clamped)\n",
        "    r_th = torch.exp(logp_t - logp_old_t)\n",
        "    L_epsilon = -torch.min(r_th*adv_t.detach(),\n",
        "                           torch.clamp(r_th, 1.0 - e_clip, 1.0 + e_clip)*adv_t.detach()).mean()\n",
        "    actor_update.zero_grad(); L_epsilon.backward(); actor_update.step()\n",
        "\n",
        "    pi_loss_val = float(L_epsilon.item())\n",
        "    v_loss_val  = float(value_loss.item())\n",
        "\n",
        "    elapsed = time.time() - run_start_time\n",
        "    if mean_trajreward > best_mean_return:\n",
        "        best_mean_return = mean_trajreward\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": net.state_dict(),\n",
        "                \"actor_opt\": actor_update.state_dict(),\n",
        "                \"critic_opt\": critic_update.state_dict(),\n",
        "                \"epoch\": k,\n",
        "                \"best_mean_return\": best_mean_return,\n",
        "            },\n",
        "            \"actor_critic_best2.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "    f\"epoch {k+1:03d}/{num_epochs} | steps(epoch) ~{len(epoch_returns):5d} \"\n",
        "    f\"| return μ {mean_trajreward:6.2f} ± {std_trajreward:5.2f} (min {min_trajreward:5.1f}, max {max_trajreward:5.1f}) \"\n",
        "    f\"| pi_loss {pi_loss_val:7.4f} | v_loss {v_loss_val:7.4f} \"\n",
        "    f\"| best μ {best_mean_return:6.2f} | {elapsed:6.1f}s\")\n",
        "    epoch_end_idx = len(all_traj_returns)   # one past the last traj index of this epoch\n",
        "    epoch_mean_for_plot = float(np.mean(all_traj_returns[epoch_start_idx:epoch_end_idx])) if epoch_end_idx > epoch_start_idx else np.nan\n",
        "    epoch_spans.append((epoch_start_idx, epoch_end_idx, epoch_mean_for_plot))\n",
        "\n",
        "# x-axis = global trajectory index\n",
        "x = np.arange(len(all_traj_returns))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1) per-trajectory returns (blue)\n",
        "traj_line, = plt.plot(x, all_traj_returns, linewidth=1.0, label=\"trajectory return\")\n",
        "\n",
        "# 2) per-epoch mean as a red step line (built from epoch_spans)\n",
        "xs, ys = [], []\n",
        "for (s, e, m) in epoch_spans:\n",
        "    if e > s and np.isfinite(m):\n",
        "        xs += [s, e]\n",
        "        ys += [m, m]\n",
        "epoch_step = None\n",
        "if xs:\n",
        "    epoch_step = plt.step(xs, ys, where=\"post\", color=\"red\", linewidth=2.5, label=\"epoch mean return\")\n",
        "\n",
        "# (optional) faint epoch boundaries — no legend for these\n",
        "for (s, e, _) in epoch_spans:\n",
        "    plt.axvline(e - 0.5, alpha=0.1, linewidth=1)\n",
        "\n",
        "# 3) mark best epoch mean with a black star\n",
        "best_scatter = None\n",
        "valid = [(i, s, e, m) for i, (s, e, m) in enumerate(epoch_spans) if np.isfinite(m) and e > s]\n",
        "if valid:\n",
        "    i_best, s_best, e_best, m_best = max(valid, key=lambda t: t[3])\n",
        "    x_best = 0.5 * (s_best + e_best - 1)\n",
        "    best_scatter = plt.scatter([x_best], [m_best], marker='*', s=140, color='black', zorder=6, label='best epoch mean')\n",
        "    plt.annotate(f\"{m_best:.1f}\", xy=(x_best, m_best), xytext=(8, 8),\n",
        "                 textcoords=\"offset points\", fontsize=9,\n",
        "                 arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
        "\n",
        "plt.xlabel(\"Trajectory index (across all epochs)\")\n",
        "plt.ylabel(\"Return (sum of rewards)\")\n",
        "plt.title(\"Per-trajectory returns (blue) with per-epoch mean (red step)\")\n",
        "plt.legend()  # uses labels set above\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-3UJy2JjoX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"HalfCheetah-v5\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_space = env.action_space\n",
        "total_timesteps = 4000 #timesetps taken in each epoch\n",
        "traj_max_timesteps = 1001\n",
        "num_epochs = 40\n",
        "num_GAE_terms = 150\n",
        "stepsize_w = 1e-3\n",
        "stepsize_theta = 3e-4\n",
        "run_start_time = time.time()\n",
        "best_mean_return = -float(\"inf\")\n",
        "net = actor_critic_cts(state_dim, action_dim = action_space.shape[0], hidden=128, continuous = True, action_low=action_space.low, action_high=action_space.high, init_logstd=-1.0).to(\"cpu\")\n",
        "\n",
        "w = list(net.v_net.parameters())\n",
        "e_clip = 0.2\n",
        "\n",
        "\"\"\"\n",
        "SGD(lr=0.5) on both actor and critic is extremely high for VPG with neural nets\n",
        "and categorical policies, and plain SGD here is brittle.\n",
        "Switching to Adam with small lrs is a vanilla change (not clipping/normalization)\n",
        "and typically the difference between crawling and clean convergence.\n",
        "\"\"\"\n",
        "actor_params = list(net.pi_net.parameters())\n",
        "if net.continuous:\n",
        "    actor_params += list(net.mu_head.parameters()) + [net.log_std]\n",
        "else:\n",
        "    actor_params += list(net.pi_head.parameters())\n",
        "actor_update = torch.optim.Adam(actor_params, lr=stepsize_theta)\n",
        "\n",
        "critic_update = torch.optim.Adam(w, lr=stepsize_w)\n",
        "\n",
        "all_traj_returns = []          # mean reward per trajectory (sum/length)\n",
        "all_traj_epochs = []         # epoch index for each trajectory\n",
        "epoch_spans = []\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    epoch_states, epoch_actions, epoch_vals, epoch_adv, epoch_returns = [], [], [], [], [] # data for each trajectory roll-out\n",
        "    epoch_t = 0;\n",
        "    epoch_logp = []\n",
        "    traj_returns = []\n",
        "    while epoch_t < total_timesteps:\n",
        "      s = reset_state(env)\n",
        "      traj_states, traj_actions, traj_vals, traj_rews, traj_ends = [], [], [], [], [] # data for each trajectory roll-out\n",
        "      done = False\n",
        "      traj_t = 0\n",
        "      epoch_start_idx = len(all_traj_returns)\n",
        "      while (not done) and (traj_t<traj_max_timesteps):\n",
        "        s = torch.as_tensor(s, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          dist, V = net(s)\n",
        "          a = dist.sample()\n",
        "          logp_a = dist.log_prob(a)\n",
        "        a_env = a.squeeze(0).cpu().numpy().astype(np.float32)  #AI\n",
        "        s_next, r, terminated, truncated, _ = env.step(a_env)\n",
        "        done = terminated or truncated #use this instead of a defined for loop, because in gymnasium the trajectory might terminate in the environment when some success/failure conditions are met\n",
        "        traj_rews.append(r)\n",
        "        traj_states.append(s.squeeze(0))\n",
        "        traj_vals.append(V.item())\n",
        "        traj_actions.append(a.squeeze(0))\n",
        "        epoch_logp.append(logp_a.squeeze(0))\n",
        "        \"\"\"terminated = true environment terminal (pole fell) → mask it.\n",
        "            truncated = time limit → do not mask; bootstrap with V_sT\n",
        "        \"\"\"\n",
        "        traj_ends.append(1.0 if terminated else 0.0)\n",
        "        s = s_next\n",
        "        traj_t += 1\n",
        "        epoch_t += 1\n",
        "      # AI IDENTIFIED FIXES\n",
        "        if epoch_t >= total_timesteps:      # FIX: stop collecting if epoch budget reached\n",
        "            break\n",
        "      # ---- prevent ends[-1] crash if trajectory collected 0 steps ----\n",
        "      if len(traj_rews) == 0:               # FIX: skip empty trajectories\n",
        "        continue\n",
        "      rews  = torch.tensor(traj_rews,  dtype=torch.float32)\n",
        "      vals  = torch.tensor(traj_vals,  dtype=torch.float32)\n",
        "      ends = torch.tensor(traj_ends, dtype=torch.float32)\n",
        "\n",
        "      if ends[-1] == 1.0:  #implementation and use of dones to handle the ends of trajectory by truncation or termination was implemented with ChatGPT 5 assistance\n",
        "        next_v = 0.0\n",
        "      else:\n",
        "        s_tplus1 = torch.as_tensor(s, dtype=torch.float32, device=\"cpu\").unsqueeze(0) #obtain the state after the last action taken before trajectory ended\n",
        "        with torch.no_grad(): #AI provided. Used so that our value function is a constant, and estimated strictly through NN output, and does not give along the graph\n",
        "                              #that generated the next_v\n",
        "                              #using .item() does ensure in part that the graph is not given along,\n",
        "                              #but no_grad() allows us to have entirely skip the computation of the generation graph\n",
        "            next_v = net.value(s_tplus1).item()\n",
        "\n",
        "      A = GAE(traj_rews, traj_vals, next_v, traj_ends)\n",
        "      ret = A + torch.tensor(traj_vals, dtype=torch.float32)\n",
        "\n",
        "      epoch_states.extend(traj_states)\n",
        "      epoch_actions.extend(traj_actions)\n",
        "      epoch_vals.extend(traj_vals)\n",
        "      epoch_adv.extend(A.tolist())\n",
        "      epoch_returns.extend(ret.tolist())\n",
        "\n",
        "      traj_len = len(traj_rews)\n",
        "      # after a trajectory finishes\n",
        "      traj_return = rews.sum().item()             # <-- sum, not mean\n",
        "      traj_returns.append(traj_return)\n",
        "\n",
        "      all_traj_returns.append(traj_return)        # rename your list\n",
        "      all_traj_epochs.append(k)\n",
        "\n",
        "    if len(epoch_states) == 0:\n",
        "      print(\"empty epoch; continuing\")\n",
        "      continue\n",
        "\n",
        "    mean_trajreward = float(np.mean(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    std_trajreward  = float(np.std(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    min_trajreward  = float(np.min(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    max_trajreward  = float(np.max(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "\n",
        "    state_t = torch.stack(epoch_states).to(torch.float32)\n",
        "    action_t = torch.stack(epoch_actions).to(torch.float32)\n",
        "    adv_t = torch.tensor(epoch_adv, dtype=torch.float32)\n",
        "    ret_t = torch.tensor(epoch_returns, dtype=torch.float32)\n",
        "    logp_old_t = torch.tensor(epoch_logp, dtype=torch.float32)\n",
        "\n",
        "    v_step = net.value(state_t).squeeze(-1)                                  # V(s_t) (with grad)\n",
        "    value_loss = torch.nn.functional.mse_loss(v_step, ret_t.detach())\n",
        "    critic_update.zero_grad(); value_loss.backward(); critic_update.step()\n",
        "\n",
        "    # Clamp actions into open interval so atanh is well-defined when re-computing log_prob\n",
        "    ###AIstart\n",
        "    eps = 1e-6\n",
        "    low  = (net.action_loc - net.action_scale + eps).unsqueeze(0)\n",
        "    high = (net.action_loc + net.action_scale - eps).unsqueeze(0)\n",
        "    action_t_clamped = torch.max(torch.min(action_t, high), low)\n",
        "    ###AIend\n",
        "\n",
        "    dist = net.policy(state_t)\n",
        "    logp_t = dist.log_prob(action_t_clamped)\n",
        "    r_th = torch.exp(logp_t - logp_old_t)\n",
        "    L_epsilon = -torch.min(r_th*adv_t.detach(),\n",
        "                           torch.clamp(r_th, 1.0 - e_clip, 1.0 + e_clip)*adv_t.detach()).mean()\n",
        "    actor_update.zero_grad(); L_epsilon.backward(); actor_update.step()\n",
        "\n",
        "    pi_loss_val = float(L_epsilon.item())\n",
        "    v_loss_val  = float(value_loss.item())\n",
        "\n",
        "    elapsed = time.time() - run_start_time\n",
        "    if mean_trajreward > best_mean_return:\n",
        "        best_mean_return = mean_trajreward\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": net.state_dict(),\n",
        "                \"actor_opt\": actor_update.state_dict(),\n",
        "                \"critic_opt\": critic_update.state_dict(),\n",
        "                \"epoch\": k,\n",
        "                \"best_mean_return\": best_mean_return,\n",
        "            },\n",
        "            \"actor_critic_best3.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "    f\"epoch {k+1:03d}/{num_epochs} | steps(epoch) ~{len(epoch_returns):5d} \"\n",
        "    f\"| return μ {mean_trajreward:6.2f} ± {std_trajreward:5.2f} (min {min_trajreward:5.1f}, max {max_trajreward:5.1f}) \"\n",
        "    f\"| pi_loss {pi_loss_val:7.4f} | v_loss {v_loss_val:7.4f} \"\n",
        "    f\"| best μ {best_mean_return:6.2f} | {elapsed:6.1f}s\")\n",
        "    epoch_end_idx = len(all_traj_returns)   # one past the last traj index of this epoch\n",
        "    epoch_mean_for_plot = float(np.mean(all_traj_returns[epoch_start_idx:epoch_end_idx])) if epoch_end_idx > epoch_start_idx else np.nan\n",
        "    epoch_spans.append((epoch_start_idx, epoch_end_idx, epoch_mean_for_plot))\n",
        "\n",
        "# x-axis = global trajectory index\n",
        "x = np.arange(len(all_traj_returns))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1) per-trajectory returns (blue)\n",
        "traj_line, = plt.plot(x, all_traj_returns, linewidth=1.0, label=\"trajectory return\")\n",
        "\n",
        "# 2) per-epoch mean as a red step line (built from epoch_spans)\n",
        "xs, ys = [], []\n",
        "for (s, e, m) in epoch_spans:\n",
        "    if e > s and np.isfinite(m):\n",
        "        xs += [s, e]\n",
        "        ys += [m, m]\n",
        "epoch_step = None\n",
        "if xs:\n",
        "    epoch_step = plt.step(xs, ys, where=\"post\", color=\"red\", linewidth=2.5, label=\"epoch mean return\")\n",
        "\n",
        "# (optional) faint epoch boundaries — no legend for these\n",
        "for (s, e, _) in epoch_spans:\n",
        "    plt.axvline(e - 0.5, alpha=0.1, linewidth=1)\n",
        "\n",
        "# 3) mark best epoch mean with a black star\n",
        "best_scatter = None\n",
        "valid = [(i, s, e, m) for i, (s, e, m) in enumerate(epoch_spans) if np.isfinite(m) and e > s]\n",
        "if valid:\n",
        "    i_best, s_best, e_best, m_best = max(valid, key=lambda t: t[3])\n",
        "    x_best = 0.5 * (s_best + e_best - 1)\n",
        "    best_scatter = plt.scatter([x_best], [m_best], marker='*', s=140, color='black', zorder=6, label='best epoch mean')\n",
        "    plt.annotate(f\"{m_best:.1f}\", xy=(x_best, m_best), xytext=(8, 8),\n",
        "                 textcoords=\"offset points\", fontsize=9,\n",
        "                 arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
        "\n",
        "plt.xlabel(\"Trajectory index (across all epochs)\")\n",
        "plt.ylabel(\"Return (sum of rewards)\")\n",
        "plt.title(\"Per-trajectory returns (blue) with per-epoch mean (red step)\")\n",
        "plt.legend()  # uses labels set above\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vT9dhFehddmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Swimmer-v5\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_space = env.action_space\n",
        "total_timesteps = 4000 #timesetps taken in each epoch\n",
        "traj_max_timesteps = 1001\n",
        "num_epochs = 40\n",
        "num_GAE_terms = 150\n",
        "stepsize_w = 1e-3\n",
        "stepsize_theta = 3e-4\n",
        "run_start_time = time.time()\n",
        "best_mean_return = -float(\"inf\")\n",
        "net = actor_critic_cts(state_dim, action_dim = action_space.shape[0], hidden=128, continuous = True, action_low=action_space.low, action_high=action_space.high).to(\"cpu\")\n",
        "\n",
        "w = list(net.v_net.parameters())\n",
        "e_clip = 0.2\n",
        "\n",
        "\"\"\"\n",
        "SGD(lr=0.5) on both actor and critic is extremely high for VPG with neural nets\n",
        "and categorical policies, and plain SGD here is brittle.\n",
        "Switching to Adam with small lrs is a vanilla change (not clipping/normalization)\n",
        "and typically the difference between crawling and clean convergence.\n",
        "\"\"\"\n",
        "actor_params = list(net.pi_net.parameters())\n",
        "if net.continuous:\n",
        "    actor_params += list(net.mu_head.parameters()) + [net.log_std]\n",
        "else:\n",
        "    actor_params += list(net.pi_head.parameters())\n",
        "actor_update = torch.optim.Adam(actor_params, lr=stepsize_theta)\n",
        "theta = actor_params\n",
        "critic_update = torch.optim.Adam(w, lr=stepsize_w)\n",
        "\n",
        "all_traj_returns = []          # mean reward per trajectory (sum/length)\n",
        "all_traj_epochs = []         # epoch index for each trajectory\n",
        "epoch_spans = []\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    epoch_states, epoch_actions, epoch_vals, epoch_adv, epoch_returns = [], [], [], [], [] # data for each trajectory roll-out\n",
        "    epoch_t = 0;\n",
        "    epoch_logp = []\n",
        "    traj_returns = []\n",
        "    while epoch_t < total_timesteps:\n",
        "      s = reset_state(env)\n",
        "      traj_states, traj_actions, traj_vals, traj_rews, traj_ends = [], [], [], [], [] # data for each trajectory roll-out\n",
        "      done = False\n",
        "      traj_t = 0\n",
        "      epoch_start_idx = len(all_traj_returns)\n",
        "      while (not done) and (traj_t<traj_max_timesteps):\n",
        "        s = torch.as_tensor(s, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          dist, V = net(s)\n",
        "          a = dist.sample()\n",
        "          logp_a = dist.log_prob(a).sum(-1)\n",
        "        a_env = a.squeeze(0).cpu().numpy().astype(np.float32)  #AI\n",
        "        s_next, r, terminated, truncated, _ = env.step(a_env)\n",
        "        done = terminated or truncated #use this instead of a defined for loop, because in gymnasium the trajectory might terminate in the environment when some success/failure conditions are met\n",
        "        traj_rews.append(r)\n",
        "        traj_states.append(s.squeeze(0))\n",
        "        traj_vals.append(V.item())\n",
        "        traj_actions.append(a.squeeze(0))\n",
        "        epoch_logp.append(logp_a.squeeze(0))\n",
        "        \"\"\"terminated = true environment terminal (pole fell) → mask it.\n",
        "            truncated = time limit → do not mask; bootstrap with V_sT\n",
        "        \"\"\"\n",
        "        traj_ends.append(1.0 if terminated else 0.0)\n",
        "        s = s_next\n",
        "        traj_t += 1\n",
        "        epoch_t += 1\n",
        "      # AI IDENTIFIED FIXES\n",
        "        if epoch_t >= total_timesteps:      # FIX: stop collecting if epoch budget reached\n",
        "            break\n",
        "      # ---- prevent ends[-1] crash if trajectory collected 0 steps ----\n",
        "      if len(traj_rews) == 0:               # FIX: skip empty trajectories\n",
        "        continue\n",
        "      rews  = torch.tensor(traj_rews,  dtype=torch.float32)\n",
        "      vals  = torch.tensor(traj_vals,  dtype=torch.float32)\n",
        "      ends = torch.tensor(traj_ends, dtype=torch.float32)\n",
        "\n",
        "      if ends[-1] == 1.0:  #implementation and use of dones to handle the ends of trajectory by truncation or termination was implemented with ChatGPT 5 assistance\n",
        "        next_v = 0.0\n",
        "      else:\n",
        "        s_tplus1 = torch.as_tensor(s, dtype=torch.float32, device=\"cpu\").unsqueeze(0) #obtain the state after the last action taken before trajectory ended\n",
        "        with torch.no_grad(): #AI provided. Used so that our value function is a constant, and estimated strictly through NN output, and does not give along the graph\n",
        "                              #that generated the next_v\n",
        "                              #using .item() does ensure in part that the graph is not given along,\n",
        "                              #but no_grad() allows us to have entirely skip the computation of the generation graph\n",
        "            next_v = net.value(s_tplus1).item()\n",
        "\n",
        "      A = GAE(traj_rews, traj_vals, next_v, traj_ends)\n",
        "      ret = A + torch.tensor(traj_vals, dtype=torch.float32)\n",
        "\n",
        "      epoch_states.extend(traj_states)\n",
        "      epoch_actions.extend(traj_actions)\n",
        "      epoch_vals.extend(traj_vals)\n",
        "      epoch_adv.extend(A.tolist())\n",
        "      epoch_returns.extend(ret.tolist())\n",
        "\n",
        "      traj_len = len(traj_rews)\n",
        "      # after a trajectory finishes\n",
        "      traj_return = rews.sum().item()             # <-- sum, not mean\n",
        "      traj_returns.append(traj_return)\n",
        "\n",
        "      all_traj_returns.append(traj_return)        # rename your list\n",
        "      all_traj_epochs.append(k)\n",
        "\n",
        "    if len(epoch_states) == 0:\n",
        "      print(\"empty epoch; continuing\")\n",
        "      continue\n",
        "\n",
        "    mean_trajreward = float(np.mean(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    std_trajreward  = float(np.std(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    min_trajreward  = float(np.min(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    max_trajreward  = float(np.max(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "\n",
        "    state_t = torch.stack(epoch_states).to(torch.float32)\n",
        "    action_t = torch.stack(epoch_actions).to(torch.float32)\n",
        "    adv_t = torch.tensor(epoch_adv, dtype=torch.float32)\n",
        "    ret_t = torch.tensor(epoch_returns, dtype=torch.float32)\n",
        "    logp_old_t = torch.tensor(epoch_logp, dtype=torch.float32)\n",
        "\n",
        "    v_step = net.value(state_t).squeeze(-1)                                  # V(s_t) (with grad)\n",
        "    value_loss = torch.nn.functional.mse_loss(v_step, ret_t.detach())\n",
        "    critic_update.zero_grad(); value_loss.backward(); critic_update.step()\n",
        "\n",
        "    # Clamp actions into open interval so atanh is well-defined when re-computing log_prob\n",
        "    ###AIstart\n",
        "    eps = 1e-6\n",
        "    low  = (net.action_loc - net.action_scale + eps).unsqueeze(0)\n",
        "    high = (net.action_loc + net.action_scale - eps).unsqueeze(0)\n",
        "    action_t_clamped = torch.max(torch.min(action_t, high), low)\n",
        "    ###AIend\n",
        "\n",
        "    dist = net.policy(state_t)\n",
        "    logp_t = dist.log_prob(action_t_clamped)\n",
        "    r_th = torch.exp(logp_t - logp_old_t)\n",
        "    L_epsilon = -torch.min(r_th*adv_t.detach(),\n",
        "                           torch.clamp(r_th, 1.0 - e_clip, 1.0 + e_clip)*adv_t.detach()).mean()\n",
        "    actor_update.zero_grad(); L_epsilon.backward(); actor_update.step()\n",
        "\n",
        "    pi_loss_val = float(L_epsilon.item())\n",
        "    v_loss_val  = float(value_loss.item())\n",
        "\n",
        "    elapsed = time.time() - run_start_time\n",
        "    if mean_trajreward > best_mean_return:\n",
        "        best_mean_return = mean_trajreward\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": net.state_dict(),\n",
        "                \"actor_opt\": actor_update.state_dict(),\n",
        "                \"critic_opt\": critic_update.state_dict(),\n",
        "                \"epoch\": k,\n",
        "                \"best_mean_return\": best_mean_return,\n",
        "            },\n",
        "            \"actor_critic_best4.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "    f\"epoch {k+1:03d}/{num_epochs} | steps(epoch) ~{len(epoch_returns):5d} \"\n",
        "    f\"| return μ {mean_trajreward:6.2f} ± {std_trajreward:5.2f} (min {min_trajreward:5.1f}, max {max_trajreward:5.1f}) \"\n",
        "    f\"| pi_loss {pi_loss_val:7.4f} | v_loss {v_loss_val:7.4f} \"\n",
        "    f\"| best μ {best_mean_return:6.2f} | {elapsed:6.1f}s\")\n",
        "    epoch_end_idx = len(all_traj_returns)   # one past the last traj index of this epoch\n",
        "    epoch_mean_for_plot = float(np.mean(all_traj_returns[epoch_start_idx:epoch_end_idx])) if epoch_end_idx > epoch_start_idx else np.nan\n",
        "    epoch_spans.append((epoch_start_idx, epoch_end_idx, epoch_mean_for_plot))\n",
        "\n",
        "# x-axis = global trajectory index\n",
        "x = np.arange(len(all_traj_returns))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1) per-trajectory returns (blue)\n",
        "traj_line, = plt.plot(x, all_traj_returns, linewidth=1.0, label=\"trajectory return\")\n",
        "\n",
        "# 2) per-epoch mean as a red step line (built from epoch_spans)\n",
        "xs, ys = [], []\n",
        "for (s, e, m) in epoch_spans:\n",
        "    if e > s and np.isfinite(m):\n",
        "        xs += [s, e]\n",
        "        ys += [m, m]\n",
        "epoch_step = None\n",
        "if xs:\n",
        "    epoch_step = plt.step(xs, ys, where=\"post\", color=\"red\", linewidth=2.5, label=\"epoch mean return\")\n",
        "\n",
        "# (optional) faint epoch boundaries — no legend for these\n",
        "for (s, e, _) in epoch_spans:\n",
        "    plt.axvline(e - 0.5, alpha=0.1, linewidth=1)\n",
        "\n",
        "# 3) mark best epoch mean with a black star\n",
        "best_scatter = None\n",
        "valid = [(i, s, e, m) for i, (s, e, m) in enumerate(epoch_spans) if np.isfinite(m) and e > s]\n",
        "if valid:\n",
        "    i_best, s_best, e_best, m_best = max(valid, key=lambda t: t[3])\n",
        "    x_best = 0.5 * (s_best + e_best - 1)\n",
        "    best_scatter = plt.scatter([x_best], [m_best], marker='*', s=140, color='black', zorder=6, label='best epoch mean')\n",
        "    plt.annotate(f\"{m_best:.1f}\", xy=(x_best, m_best), xytext=(8, 8),\n",
        "                 textcoords=\"offset points\", fontsize=9,\n",
        "                 arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
        "\n",
        "plt.xlabel(\"Trajectory index (across all epochs)\")\n",
        "plt.ylabel(\"Return (sum of rewards)\")\n",
        "plt.title(\"Per-trajectory returns (blue) with per-epoch mean (red step)\")\n",
        "plt.legend()  # uses labels set above\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V5QE-YnKih9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"InvertedDoublePendulum-v5\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_space = env.action_space\n",
        "total_timesteps = 4000 #timesetps taken in each epoch\n",
        "traj_max_timesteps = 1000\n",
        "num_epochs = 60\n",
        "num_GAE_terms = 150\n",
        "stepsize_w = 1e-3\n",
        "stepsize_theta = 3e-4\n",
        "run_start_time = time.time()\n",
        "best_mean_return = -float(\"inf\")\n",
        "net = actor_critic_cts(state_dim, action_dim = action_space.shape[0], hidden=128, continuous = True, action_low=action_space.low, action_high=action_space.high).to(\"cpu\")\n",
        "\n",
        "w = list(net.v_net.parameters())\n",
        "e_clip = 0.2\n",
        "\n",
        "\"\"\"\n",
        "SGD(lr=0.5) on both actor and critic is extremely high for VPG with neural nets\n",
        "and categorical policies, and plain SGD here is brittle.\n",
        "Switching to Adam with small lrs is a vanilla change (not clipping/normalization)\n",
        "and typically the difference between crawling and clean convergence.\n",
        "\"\"\"\n",
        "actor_params = list(net.pi_net.parameters())\n",
        "if net.continuous:\n",
        "    actor_params += list(net.mu_head.parameters()) + [net.log_std]\n",
        "else:\n",
        "    actor_params += list(net.pi_head.parameters())\n",
        "actor_update = torch.optim.Adam(actor_params, lr=stepsize_theta)\n",
        "\n",
        "critic_update = torch.optim.Adam(w, lr=stepsize_w)\n",
        "\n",
        "all_traj_returns = []          # mean reward per trajectory (sum/length)\n",
        "all_traj_epochs = []         # epoch index for each trajectory\n",
        "epoch_spans = []\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    epoch_states, epoch_actions, epoch_vals, epoch_adv, epoch_returns = [], [], [], [], [] # data for each trajectory roll-out\n",
        "    epoch_t = 0;\n",
        "    epoch_logp = []\n",
        "    traj_returns = []\n",
        "    while epoch_t < total_timesteps:\n",
        "      s = reset_state(env)\n",
        "      traj_states, traj_actions, traj_vals, traj_rews, traj_ends = [], [], [], [], [] # data for each trajectory roll-out\n",
        "      done = False\n",
        "      traj_t = 0\n",
        "      epoch_start_idx = len(all_traj_returns)\n",
        "      while (not done) and (traj_t<traj_max_timesteps):\n",
        "        s = torch.as_tensor(s, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          dist, V = net(s)\n",
        "          a = dist.sample()\n",
        "          logp_a = dist.log_prob(a).sum(-1)\n",
        "        a_env = a.squeeze(0).cpu().numpy().astype(np.float32)  #AI\n",
        "        s_next, r, terminated, truncated, _ = env.step(a_env)\n",
        "        done = terminated or truncated #use this instead of a defined for loop, because in gymnasium the trajectory might terminate in the environment when some success/failure conditions are met\n",
        "        traj_rews.append(r)\n",
        "        traj_states.append(s.squeeze(0))\n",
        "        traj_vals.append(V.item())\n",
        "        traj_actions.append(a.squeeze(0))\n",
        "        epoch_logp.append(logp_a.squeeze(0))\n",
        "        \"\"\"terminated = true environment terminal (pole fell) → mask it.\n",
        "            truncated = time limit → do not mask; bootstrap with V_sT\n",
        "        \"\"\"\n",
        "        traj_ends.append(1.0 if terminated else 0.0)\n",
        "        s = s_next\n",
        "        traj_t += 1\n",
        "        epoch_t += 1\n",
        "      # AI IDENTIFIED FIXES\n",
        "        if epoch_t >= total_timesteps:      # FIX: stop collecting if epoch budget reached\n",
        "            break\n",
        "      # ---- prevent ends[-1] crash if trajectory collected 0 steps ----\n",
        "      if len(traj_rews) == 0:               # FIX: skip empty trajectories\n",
        "        continue\n",
        "      rews  = torch.tensor(traj_rews,  dtype=torch.float32)\n",
        "      vals  = torch.tensor(traj_vals,  dtype=torch.float32)\n",
        "      ends = torch.tensor(traj_ends, dtype=torch.float32)\n",
        "\n",
        "      if ends[-1] == 1.0:  #implementation and use of dones to handle the ends of trajectory by truncation or termination was implemented with ChatGPT 5 assistance\n",
        "        next_v = 0.0\n",
        "      else:\n",
        "        s_tplus1 = torch.as_tensor(s, dtype=torch.float32, device=\"cpu\").unsqueeze(0) #obtain the state after the last action taken before trajectory ended\n",
        "        with torch.no_grad(): #AI provided. Used so that our value function is a constant, and estimated strictly through NN output, and does not give along the graph\n",
        "                              #that generated the next_v\n",
        "                              #using .item() does ensure in part that the graph is not given along,\n",
        "                              #but no_grad() allows us to have entirely skip the computation of the generation graph\n",
        "            next_v = net.value(s_tplus1).item()\n",
        "\n",
        "      A = GAE(traj_rews, traj_vals, next_v, traj_ends)\n",
        "      ret = A + torch.tensor(traj_vals, dtype=torch.float32)\n",
        "\n",
        "      epoch_states.extend(traj_states)\n",
        "      epoch_actions.extend(traj_actions)\n",
        "      epoch_vals.extend(traj_vals)\n",
        "      epoch_adv.extend(A.tolist())\n",
        "      epoch_returns.extend(ret.tolist())\n",
        "\n",
        "      traj_len = len(traj_rews)\n",
        "      # after a trajectory finishes\n",
        "      traj_return = rews.sum().item()             # <-- sum, not mean\n",
        "      traj_returns.append(traj_return)\n",
        "\n",
        "      all_traj_returns.append(traj_return)        # rename your list\n",
        "      all_traj_epochs.append(k)\n",
        "\n",
        "    if len(epoch_states) == 0:\n",
        "      print(\"empty epoch; continuing\")\n",
        "      continue\n",
        "\n",
        "    mean_trajreward = float(np.mean(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    std_trajreward  = float(np.std(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    min_trajreward  = float(np.min(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "    max_trajreward  = float(np.max(traj_returns))  if len(traj_returns) > 0 else 0.0\n",
        "\n",
        "    state_t = torch.stack(epoch_states).to(torch.float32)\n",
        "    action_t = torch.stack(epoch_actions).to(torch.float32)\n",
        "    adv_t = torch.tensor(epoch_adv, dtype=torch.float32)\n",
        "    ret_t = torch.tensor(epoch_returns, dtype=torch.float32)\n",
        "    logp_old_t = torch.tensor(epoch_logp, dtype=torch.float32)\n",
        "\n",
        "    v_step = net.value(state_t).squeeze(-1)                                  # V(s_t) (with grad)\n",
        "    value_loss = torch.nn.functional.mse_loss(v_step, ret_t.detach())\n",
        "    critic_update.zero_grad(); value_loss.backward(); critic_update.step()\n",
        "\n",
        "    # Clamp actions into open interval so atanh is well-defined when re-computing log_prob\n",
        "    ###AIstart\n",
        "    eps = 1e-6\n",
        "    low  = (net.action_loc - net.action_scale + eps).unsqueeze(0)\n",
        "    high = (net.action_loc + net.action_scale - eps).unsqueeze(0)\n",
        "    action_t_clamped = torch.max(torch.min(action_t, high), low)\n",
        "    ###AIend\n",
        "\n",
        "    dist = net.policy(state_t)\n",
        "    logp_t = dist.log_prob(action_t_clamped)\n",
        "    r_th = torch.exp(logp_t - logp_old_t)\n",
        "    L_epsilon = -torch.min(r_th*adv_t.detach(),\n",
        "                           torch.clamp(r_th, 1.0 - e_clip, 1.0 + e_clip)*adv_t.detach()).mean()\n",
        "    actor_update.zero_grad(); L_epsilon.backward(); actor_update.step()\n",
        "\n",
        "    pi_loss_val = float(L_epsilon.item())\n",
        "    v_loss_val  = float(value_loss.item())\n",
        "\n",
        "    elapsed = time.time() - run_start_time\n",
        "    if mean_trajreward > best_mean_return:\n",
        "        best_mean_return = mean_trajreward\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": net.state_dict(),\n",
        "                \"actor_opt\": actor_update.state_dict(),\n",
        "                \"critic_opt\": critic_update.state_dict(),\n",
        "                \"epoch\": k,\n",
        "                \"best_mean_return\": best_mean_return,\n",
        "            },\n",
        "            \"actor_critic_best5.pt\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "    f\"epoch {k+1:03d}/{num_epochs} | steps(epoch) ~{len(epoch_returns):5d} \"\n",
        "    f\"| return μ {mean_trajreward:6.2f} ± {std_trajreward:5.2f} (min {min_trajreward:5.1f}, max {max_trajreward:5.1f}) \"\n",
        "    f\"| pi_loss {pi_loss_val:7.4f} | v_loss {v_loss_val:7.4f} \"\n",
        "    f\"| best μ {best_mean_return:6.2f} | {elapsed:6.1f}s\")\n",
        "    epoch_end_idx = len(all_traj_returns)   # one past the last traj index of this epoch\n",
        "    epoch_mean_for_plot = float(np.mean(all_traj_returns[epoch_start_idx:epoch_end_idx])) if epoch_end_idx > epoch_start_idx else np.nan\n",
        "    epoch_spans.append((epoch_start_idx, epoch_end_idx, epoch_mean_for_plot))\n",
        "\n",
        "# x-axis = global trajectory index\n",
        "x = np.arange(len(all_traj_returns))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1) per-trajectory returns (blue)\n",
        "traj_line, = plt.plot(x, all_traj_returns, linewidth=1.0, label=\"trajectory return\")\n",
        "\n",
        "# 2) per-epoch mean as a red step line (built from epoch_spans)\n",
        "xs, ys = [], []\n",
        "for (s, e, m) in epoch_spans:\n",
        "    if e > s and np.isfinite(m):\n",
        "        xs += [s, e]\n",
        "        ys += [m, m]\n",
        "epoch_step = None\n",
        "if xs:\n",
        "    epoch_step = plt.step(xs, ys, where=\"post\", color=\"red\", linewidth=2.5, label=\"epoch mean return\")\n",
        "\n",
        "# (optional) faint epoch boundaries — no legend for these\n",
        "for (s, e, _) in epoch_spans:\n",
        "    plt.axvline(e - 0.5, alpha=0.1, linewidth=1)\n",
        "\n",
        "# 3) mark best epoch mean with a black star\n",
        "best_scatter = None\n",
        "valid = [(i, s, e, m) for i, (s, e, m) in enumerate(epoch_spans) if np.isfinite(m) and e > s]\n",
        "if valid:\n",
        "    i_best, s_best, e_best, m_best = max(valid, key=lambda t: t[3])\n",
        "    x_best = 0.5 * (s_best + e_best - 1)\n",
        "    best_scatter = plt.scatter([x_best], [m_best], marker='*', s=140, color='black', zorder=6, label='best epoch mean')\n",
        "    plt.annotate(f\"{m_best:.1f}\", xy=(x_best, m_best), xytext=(8, 8),\n",
        "                 textcoords=\"offset points\", fontsize=9,\n",
        "                 arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
        "\n",
        "plt.xlabel(\"Trajectory index (across all epochs)\")\n",
        "plt.ylabel(\"Return (sum of rewards)\")\n",
        "plt.title(\"Per-trajectory returns (blue) with per-epoch mean (red step)\")\n",
        "plt.legend()  # uses labels set above\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UMxWf9lF3ePT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
